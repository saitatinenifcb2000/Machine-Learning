{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import numpy as np\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = 'I want to watch a movie this weekend.'\n",
    "D2 =  'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.'\n",
    "D3 =  'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.'\n",
    "D4 =  'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!'\n",
    "D5 =  'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I want to watch a movie this weekend.',\n",
       " 'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.',\n",
       " 'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.',\n",
       " 'Movies are a nice way to chill however, this time I would like to paint and read some good books. It’s been long!',\n",
       " 'This blueberry milkshake is so good! Try reading Dr. Joe Dispenza’s books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [D1, D2, D3, D4, D5]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "print(len(stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'under', 'now', 'doing', 'can', 'further', 'ma', 'themselves', 'again', 'hers', 'herself', 'an', 'we', 'here', 'by', 'through', 'from', 'doesn', 'ain', 'yourselves', 'wasn', 's', 'as', \"doesn't\", 'hasn', 'i', 'same', 'nor', 'll', 'for', 'once', 'o', 'all', 'any', 'whom', 'needn', 'shouldn', 'theirs', 'wouldn', \"should've\", 'mightn', 'because', 'against', 'y', 'won', 'were', 'myself', 'to', \"you'd\", 'didn', 'was', 've', 'his', 'what', 'such', \"hadn't\", 'mustn', 'their', 'who', 'those', 'down', 'after', 'yourself', 'aren', \"weren't\", 'before', 'do', 'into', 'the', 'each', 'don', 'her', 'few', 'too', \"didn't\", 'been', \"shouldn't\", 'shan', 'is', 'these', \"you've\", 'up', 'between', 'in', 'not', 'just', 'isn', 'hadn', 'you', 'below', 'should', 'then', 'why', 'which', \"don't\", 'but', 'while', 'more', \"she's\", 'at', 'own', 'its', 'does', 'where', \"needn't\", 'a', \"hasn't\", 'be', 't', 'weren', 'off', 'my', 'only', 'he', 'some', 'when', 'had', 'him', 'himself', 'they', 'them', 'other', \"won't\", 'itself', \"mustn't\", \"wasn't\", 'am', \"haven't\", 'most', \"it's\", 'there', 'd', 'it', 'are', 'did', 'and', \"you'll\", 'have', 'over', \"that'll\", 'haven', \"mightn't\", 'our', 'will', 're', 'very', 'than', 'm', \"aren't\", \"shan't\", 'has', 'both', 'with', 'ourselves', 'until', 'your', \"couldn't\", 'or', 'above', 'about', 'so', 'during', 'on', 'this', \"wouldn't\", \"isn't\", 'of', \"you're\", 'how', 'ours', 'couldn', 'me', 'that', 'yours', 'out', 'no', 'if', 'being', 'having', 'she'} "
     ]
    }
   ],
   "source": [
    "print(stop, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\\\', '%', '[', \"'\", '{', ')', '=', '-', '$', '|', '/', '\"', '*', '}', '<', ';', ':', '(', '?', '@', '!', ']', '^', '~', '&', '`', '+', ',', '>', '.', '_', '#'} "
     ]
    }
   ],
   "source": [
    "print(set(string.punctuation), end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'want', 'to', 'watch', 'a', 'movie', 'this', 'weekend']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"I want to watch a movie this weekend\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "exclude = set(string.punctuation) \n",
    "\n",
    "stem = PorterStemmer()\n",
    "\n",
    "# One function for all the steps:\n",
    "def clean(doc):\n",
    "    \n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)  \n",
    "    normalized = \" \".join(stem.stem(word) for word in punc_free.split())  \n",
    "    return normalized\n",
    "\n",
    "# clean data stored in a new list\n",
    "clean_corpus = [clean(doc).split() for doc in corpus]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['want', 'watch', 'movi', 'weekend'], ['went', 'shop', 'yesterday', 'new', 'zealand', 'world', 'test', 'championship', 'beat', 'india', 'eight', 'wicket', 'southampton'], ['don’t', 'watch', 'cricket', 'netflix', 'amazon', 'prime', 'good', 'movi', 'watch'], ['movi', 'nice', 'way', 'chill', 'howev', 'time', 'would', 'like', 'paint', 'read', 'good', 'book', 'it’', 'long'], ['blueberri', 'milkshak', 'good', 'tri', 'read', 'dr', 'joe', 'dispenza’', 'book', 'work', 'gamechang', 'book', 'help', 'learn', 'much', 'thought', 'impact', 'biolog', 'rewir', 'brain']] "
     ]
    }
   ],
   "source": [
    "print(clean_corpus, end = \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(51 unique tokens: ['movi', 'want', 'watch', 'weekend', 'beat']...)\n"
     ]
    }
   ],
   "source": [
    "dict_ = corpora.Dictionary(clean_corpus)\n",
    "print(dict_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movi, want, watch, weekend, beat, championship, eight, india, new, shop, southampton, test, went, wicket, world, yesterday, zealand, amazon, cricket, don’t, good, netflix, prime, book, chill, howev, it’, like, long, nice, paint, read, time, way, would, biolog, blueberri, brain, dispenza’, dr, gamechang, help, impact, joe, learn, milkshak, much, rewir, thought, tri, work, "
     ]
    }
   ],
   "source": [
    "for i in dict_.values():\n",
    "    print(i, end = \", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1), (3, 1)],\n",
       " [(4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 1),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1)],\n",
       " [(0, 1), (2, 2), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1)],\n",
       " [(0, 1),\n",
       "  (20, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 1),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 1),\n",
       "  (33, 1),\n",
       "  (34, 1)],\n",
       " [(20, 1),\n",
       "  (23, 2),\n",
       "  (31, 1),\n",
       "  (35, 1),\n",
       "  (36, 1),\n",
       "  (37, 1),\n",
       "  (38, 1),\n",
       "  (39, 1),\n",
       "  (40, 1),\n",
       "  (41, 1),\n",
       "  (42, 1),\n",
       "  (43, 1),\n",
       "  (44, 1),\n",
       "  (45, 1),\n",
       "  (46, 1),\n",
       "  (47, 1),\n",
       "  (48, 1),\n",
       "  (49, 1),\n",
       "  (50, 1)]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_term_matrix = [dict_.doc2bow(i) for i in clean_corpus]\n",
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "\n",
    "Lda = gensim.models.ldamodel.LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running and Training LDA model on the document term matrix.\n",
    "\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=6, id2word = dict_, passes=1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.079*\"watch\" + 0.042*\"movi\" + 0.042*\"good\" + 0.042*\"netflix\" + 0.042*\"amazon\" + 0.042*\"cricket\" + 0.042*\"prime\" + 0.042*\"don’t\" + 0.036*\"world\" + 0.036*\"southampton\"'),\n",
       " (1,\n",
       "  '0.020*\"movi\" + 0.020*\"watch\" + 0.020*\"good\" + 0.020*\"weekend\" + 0.020*\"want\" + 0.020*\"cricket\" + 0.020*\"don’t\" + 0.020*\"prime\" + 0.020*\"read\" + 0.020*\"book\"'),\n",
       " (2,\n",
       "  '0.046*\"went\" + 0.039*\"new\" + 0.037*\"shop\" + 0.036*\"yesterday\" + 0.034*\"wicket\" + 0.034*\"test\" + 0.034*\"championship\" + 0.034*\"eight\" + 0.034*\"india\" + 0.032*\"zealand\"'),\n",
       " (3,\n",
       "  '0.093*\"want\" + 0.093*\"watch\" + 0.093*\"weekend\" + 0.093*\"movi\" + 0.013*\"good\" + 0.013*\"book\" + 0.013*\"don’t\" + 0.013*\"read\" + 0.013*\"cricket\" + 0.013*\"prime\"'),\n",
       " (4,\n",
       "  '0.076*\"book\" + 0.041*\"read\" + 0.041*\"good\" + 0.041*\"biolog\" + 0.041*\"impact\" + 0.041*\"learn\" + 0.041*\"thought\" + 0.041*\"blueberri\" + 0.041*\"dr\" + 0.041*\"joe\"'),\n",
       " (5,\n",
       "  '0.052*\"movi\" + 0.052*\"good\" + 0.052*\"book\" + 0.052*\"howev\" + 0.052*\"chill\" + 0.052*\"would\" + 0.052*\"nice\" + 0.052*\"way\" + 0.052*\"it’\" + 0.052*\"long\"')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.079*\"watch\" + 0.042*\"movi\" + 0.042*\"good\" + 0.042*\"netflix\" + 0.042*\"amazon\"'), (4, '0.076*\"book\" + 0.041*\"read\" + 0.041*\"good\" + 0.041*\"biolog\" + 0.041*\"learn\"'), (1, '0.020*\"movi\" + 0.020*\"watch\" + 0.020*\"good\" + 0.020*\"weekend\" + 0.020*\"want\"'), (3, '0.093*\"want\" + 0.093*\"watch\" + 0.093*\"weekend\" + 0.093*\"movi\" + 0.013*\"good\"'), (2, '0.046*\"went\" + 0.039*\"new\" + 0.037*\"shop\" + 0.036*\"yesterday\" + 0.034*\"wicket\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=5, num_words=5))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3. Interpreting Patterns from Text - Topic Modelling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
